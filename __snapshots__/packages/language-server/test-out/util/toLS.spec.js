exports['semanticTokens Tokenize "foo" with multiline token support 1'] = [
  {
    "deltaLine": 0,
    "deltaStartChar": 0,
    "length": 100,
    "tokenType": 0,
    "tokenModifiers": 0
  }
]

exports['semanticTokens Tokenize "foo" without multiline token support 1'] = [
  {
    "deltaLine": 0,
    "deltaStartChar": 0,
    "length": 100,
    "tokenType": 0,
    "tokenModifiers": 0
  }
]

exports['semanticTokens Tokenize "foo↓bar" with multiline token support 1'] = [
  {
    "deltaLine": 0,
    "deltaStartChar": 0,
    "length": 100,
    "tokenType": 0,
    "tokenModifiers": 0
  }
]

exports['semanticTokens Tokenize "foo↓bar" without multiline token support 1'] = [
  {
    "deltaLine": 0,
    "deltaStartChar": 0,
    "length": 4,
    "tokenType": 0,
    "tokenModifiers": 0
  },
  {
    "deltaLine": 1,
    "deltaStartChar": 0,
    "length": 3,
    "tokenType": 0,
    "tokenModifiers": 0
  }
]

exports['semanticTokens Tokenize "foo↓bar↓qux" with multiline token support 1'] = [
  {
    "deltaLine": 0,
    "deltaStartChar": 0,
    "length": 100,
    "tokenType": 0,
    "tokenModifiers": 0
  }
]

exports['semanticTokens Tokenize "foo↓bar↓qux" without multiline token support 1'] = [
  {
    "deltaLine": 0,
    "deltaStartChar": 0,
    "length": 4,
    "tokenType": 0,
    "tokenModifiers": 0
  },
  {
    "deltaLine": 1,
    "deltaStartChar": 0,
    "length": 4,
    "tokenType": 0,
    "tokenModifiers": 0
  },
  {
    "deltaLine": 1,
    "deltaStartChar": 0,
    "length": 3,
    "tokenType": 0,
    "tokenModifiers": 0
  }
]
